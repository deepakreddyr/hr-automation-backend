127.0.0.1 - - [05/Sep/2025 05:44:27] "GET /api/get-questions?search_id=122 HTTP/1.1" 500 -
127.0.0.1 - - [05/Sep/2025 05:44:55] "POST /api/process/122 HTTP/1.1" 200 -
127.0.0.1 - - [05/Sep/2025 05:44:55] "OPTIONS /api/process-candidates/122 HTTP/1.1" 200 -
Traceback (most recent call last):
  File "/opt/render/project/src/main.py", line 663, in process_candidates
    candidate_data = get_candidate_details(combined_resumes, jd, skills)
  File "/opt/render/project/src/aiparser.py", line 11, in get_candidate_details
    client=OpenAI(api_key=api_key)
  File "/opt/render/project/src/.venv/lib/python3.13/site-packages/openai/_client.py", line 130, in __init__
    raise OpenAIError(
        "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
    )
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
127.0.0.1 - - [05/Sep/2025 05:44:56] "POST /api/process-candidates/122 HTTP/1.1" 500 -
127.0.0.1 - - [05/Sep/2025 05:44:57] "OPTIONS /api/check-processing/122 HTTP/1.1" 200 -
127.0.0.1 - - [05/Sep/2025 05:44:58] "GET /api/check-processing/122 HTTP/1.1" 200 -

from openai import OpenAI
from scrapper import scrape
import os 
from dotenv import load_dotenv
import json

load_dotenv()
api_key=os.getenv("OPENAI_API_KEY")
def get_candidate_details(data,jd1,skills):
    
    client=OpenAI(api_key=api_key)

    system_prompt = f"""
    Evaluate a batch of candidate resumes against a given job description (JD) to identify and shortlist qualified candidates. For each candidate, calculate a match score from 0 to 100 based on relevance to the JD. Only candidates with a score above 70 should be shortlisted. Provide output in structured JSON format for each candidate, with detailed fields for shortlisted ones and a rejection reason for others.

- Match Score Calculation: Analyze each candidate's resume against the JD to calculate a match score in the range of 0 to 100.
- Data Extraction: For candidates with a match score greater than 70, extract key fields from their resumes.
- Conditional Output: Vary the structure of the JSON output based on the match score for each resume.

# Steps

1. **Calculate Match Score**: 
   - Compare the candidates' resumes with the {jd1} to determine a match score within 0-100 for each resume.
   
2. **Conditional Logic**:
   - If a match score is greater than 70, proceed to extract detailed information from the resume.
   - If a match score is 70 or below, skip data extraction and generate a rejection reason.

3. **Data Extraction** (for match scores > 70):
   - Extract and include the following fields in the JSON output for each candidate with a high score:
     - `match_score`
     - `name`
     - `phone`
     - `email`
     - `job_summary`
     - `experience in each skill` as mentioned in {skills}
     - `total_experience` in years and months
     - `relevant_experience` in specified skills in years and months

4. **Rejection Cases** (for match scores ‚â§ 70):
   - Output a JSON containing:
     - `match_score`
     - `reason_to_reject`

5. **The number of candidates resume results should strictly be equal to the number of candidates resume input(IMPORTANT).**
# Output Format

Produce the output in JSON format for each resume:
- **If match_score > 70 for a resume**:
  {{
    "match_score": [value],
    "name": "[Name]",
    "phone": "[Phone Number]",
    "email": "[Email]",
    "job_summary": "[Summary]",
    "experience_in_skills": {{
      "[Skill1]": "[Duration]",
      "[Skill2]": "[Duration]"
    }},
    "total_experience": "[Years and Months]",
    "relevant_experience": "[Years and Months]"
  }}
"""


    response = client.responses.create(
    model="gpt-4.1-mini",
    input=[
        {
        "role": "system",
        "content": [
            {
            "type": "input_text",
            "text": system_prompt
            }
        ]
        },
        {
        "role": "user",
        "content": [
            {
            "type": "input_text",
            "text": data
            }
        ]
        },
    ],
    text={
        "format": {
        "type": "text"
        }
    },
    reasoning={},
    tools=[],
    temperature=0.5,
    max_output_tokens=2048,
    top_p=1,
    store=True
    )

    try:
        raw_output = response.output[0].content[0].text.strip()

        if not raw_output:
            print("‚ùå ERROR: Empty model output")
            return []

        # Try parsing the raw output
        try:
            parsed_output = json.loads(raw_output)
        except json.JSONDecodeError:
            print("‚ö†Ô∏è WARNING: Output is not a valid JSON list or object.")
            print("üîé Raw output:", raw_output)
            return []

        # If it's a single object, wrap it in a list
        if isinstance(parsed_output, dict):
            print("‚ÑπÔ∏è INFO: Wrapping single JSON object in list.")
            parsed_output = [parsed_output]

        # Final check: Ensure it's a list
        if not isinstance(parsed_output, list):
            print("‚ùå ERROR: Parsed output is not a list.")
            return []

        print("‚úÖ Parsed JSON:", parsed_output)
        return parsed_output

    except (AttributeError, IndexError) as e:
        print("‚ùå ERROR: Unexpected response format:", e)
        print("üßæ Full response object:", response)
        return []

    


jd ='''Data Analyst
Job Description
Overview:
We are seeking a talented and detail-oriented Data Analyst to join our dynamic team. As a Data Analyst,
you will be responsible for interpreting data, analyzing results, and providing actionable insights to drive
informed decision-making across the agency. You will work closely with stakeholders to understand their
data needs, develop analytical solutions, and present findings in a clear and concise manner.
Responsibilities
Data Collection and Processing
‚Ä¢ Extract, transform, and load (ETL) data from various sources.
‚Ä¢ Clean and preprocess data to ensure accuracy and consistency.
‚Ä¢ Develop scripts and workflows to automate data collection and processing tasks.
Data Analysis and Interpretation:
‚Ä¢ Perform exploratory data analysis to uncover trends, patterns, and anomalies.
‚Ä¢ Apply statistical and analytical techniques to derive insights from complex datasets.
‚Ä¢ Conduct hypothesis testing and predictive modeling to support business objectives.
Data Visualization and Reporting
‚Ä¢ Create visually appealing and interactive dashboards, reports, and presentations.
‚Ä¢ Communicate findings and recommendations to stakeholders using data visualization tools.
‚Ä¢ Collaborate with cross-functional teams to design and deliver actionable insights.
Data Quality Assurance
‚Ä¢ Validate data accuracy, completeness, and integrity.
‚Ä¢ Identify and address data quality issues and discrepancies.
‚Ä¢ Implement data quality controls and monitoring mechanisms.
Business Intelligence and Decision Support
‚Ä¢ Partner with business units to define key performance indicators (KPIs) and metrics.
‚Ä¢ Analyze business processes and operations to identify opportunities for improvement.
‚Ä¢ Provide decision support through ad-hoc analysis and scenario modeling.
Continuous Learning and Development
‚Ä¢ Stay abreast of industry trends, best practices, and emerging technologies in data analytics.
‚Ä¢ Participate in training programs and professional development opportunities.
‚Ä¢ Share knowledge and insights with colleagues to foster a culture of continuous learning.
Requirements:
‚Ä¢ Bachelor's degree in Statistics, Mathematics, Computer Science, Economics, or related field.
Master's degree preferred.
‚Ä¢ Proven experience in data analysis, business intelligence, or related roles.
‚Ä¢ Proficiency in data analysis tools and programming languages (e.g., SQL, Python, R, etc.).
‚Ä¢ Strong analytical and problem-solving skills with attention to detail.
‚Ä¢ Excellent communication and presentation abilities.
‚Ä¢ Ability to work independently and collaboratively in a fast-paced environment.
‚Ä¢ Experience with data visualization tools (e.g., Tableau, Power BI, etc.) is a plus.
‚Ä¢ Knowledge of machine learning and data mining techniques is desirable.'''


# jd1='''MERN Stack Developer
# Job Description

# Overview:
# We are seeking a skilled and motivated MERN Stack Developer to join our growing development team. As a MERN Stack Developer, you will be responsible for designing, developing, and maintaining scalable web applications using MongoDB, Express.js, React.js, and Node.js. You will work closely with product managers, designers, and backend teams to deliver high-quality solutions that enhance our digital products and user experience.

# Responsibilities
# Full-Stack Web Development
# Develop modern, responsive, and interactive web applications using React.js.

# Build robust backend services and RESTful APIs using Node.js and Express.js.

# Design and manage schemas and queries in MongoDB.

# System Architecture and Design
# Collaborate with cross-functional teams to gather and analyze requirements.

# Architect efficient, reusable, and scalable front-end and back-end systems.

# Ensure application performance, responsiveness, and scalability.

# Code Quality and Maintenance
# Write clean, maintainable, and well-documented code.

# Conduct code reviews, unit testing, and integration testing.

# Identify and fix bugs and performance bottlenecks.

# DevOps and Deployment
# Participate in the CI/CD pipeline and deployment processes.

# Ensure applications are secure, stable, and well monitored post-deployment.

# Optimize applications for maximum speed and scalability in production.

# Collaboration and Communication
# Work closely with UI/UX designers to bring wireframes and mockups to life.

# Communicate technical information clearly to both technical and non-technical stakeholders.

# Provide ongoing support and improvements to existing applications.

# Continuous Learning and Improvement
# Stay up-to-date with the latest industry trends and technologies in web development.

# Explore and suggest new tools and technologies to improve development workflow.

# Contribute to team knowledge-sharing and best practices.

# Requirements
# Bachelor‚Äôs degree in Computer Science, Engineering, or related field. Master‚Äôs preferred.

# Proven experience in full-stack development using the MERN stack (MongoDB, Express.js, React.js, Node.js).

# Strong understanding of JavaScript (ES6+), HTML, CSS, and RESTful API development.

# Experience with state management libraries like Redux or Context API.

# Familiarity with Git, version control, and collaborative development workflows.

# Experience with deployment and cloud platforms (e.g., AWS, Heroku, Vercel) is a plus.

# Knowledge of authentication, authorization, and security best practices.

# Excellent problem-solving, debugging, and communication skills.

# Ability to work both independently and in a team-oriented, agile environment.

# '''

data='''



NISHA KUMARI¬†+91-9709352282 # nisha06k@gmail.com √Ø LinkedIn ¬ß GithubEducationSRM University, Chennai 2017 - 2021B.Tech in Information Technology 7.01/10 CGPASaraswati Vidya Mandir, Dugda 2017Higher Secondary 62.3/10 PctSaraswati Vidya Mandir, Dugda 2015Secondary 5.8/10 CGPACoursework/SkillsSQL, Power BI Service, Power Query, SQL Server, Excel, Data Visualization, Data Cleaning, Data ModellingWork ExperienceDXC Technology June 2022 ‚Äì July 2023Associate Professional Technical SupportSupporting clients with technical issues, including on-site inspections.Fixing bugs by applying updates, upgrades, and software patches.Provided Technical support for clients Remotely and solved the issue. maintaining and fixing any error, or failureGood understanding of data querying / SQL.Concentrix Services Ind Pvt Ltd June 2021 ‚Äì June 2022Technical Support AssociateHandling users over IB and OB calls. Diagnosing and dispatching faulty parts for consumer machines. Tracking and followingup with partner‚Äôs onsite service and assisting users with remote support for S/W-related issues. Taking care of Windowsinstallation and configuration.ProjectsCOVID19 Project Source CodeThe aim of the project was good predictionData Cleaning: Deleting rows with null values and columns not needed for this analysis, and converting data types,Encoding.Data Validation and Exploration: I used Python for Data Exploration(Barchart and Heatmap), separating dependent andindependent variables And splitting the data into train and testAccuracy: Used Machine learning for model training, evaluated data using test data, Feature Scaling and LogisticRegression, Linear Regression, Decision Tree, and Random Forest.SQL: SQL is also used to find some values.Result: The highest accuracy from the Decision Tree is 84.Technical Skills‚Ä¢ SQL‚Ä¢ Python‚Ä¢ Data Analysis‚Ä¢ Statistic‚Ä¢ Data Visualization‚Ä¢ Machine LearningCertificationsData Science Bootcamp August 2023 ‚Äì February 2024OdinSchoolSQL Udemy
'''

# get_candidate_details(data,jd,"python,sql")

res = []
prof = []

def shortlist_candidates(candidates, required_skills):
    if isinstance(required_skills, str):
        required_skills = [skill.strip().lower() for skill in required_skills.split(",")]
    else:
        required_skills = [skill.strip().lower() for skill in required_skills]

    shortlisted_indices = []

    for idx, candidate in enumerate(candidates):
        candidate_text = " ".join([str(x) for x in candidate]).lower()
        matched_skills = [skill for skill in required_skills if skill in candidate_text]

        if len(matched_skills) == len(required_skills):
            shortlisted_indices.append(idx+1)

    return shortlisted_indices

def scrape(data):
    global res, prof
    res = []  # Reset global list

    for i in data.split("\n"):
        res.append(i.strip())

    sliced_data = res[46:len(res)-34]

    final = []
    temp = []

    for i in sliced_data:
        if "active" in i.lower():
            final.append(temp)
            temp = []
        elif i in ["View phone number", "Call candidate", "Verified phone & email", "\n", ""]:
            continue
        else:
            temp.append(i)

    return final




from openai import OpenAI

def get_questions(jd):
    client=OpenAI(api_key=api_key)
    prompt=f"""Generate HR-level interview questions from a provided job description to confirm a candidate's experience and qualifications. Focus on evaluating demonstrated work related to the key skills and experiences mentioned within the job description.\n\n[Frame questions based on the provided job description template.]\n\n# Steps\n\n1. Analyze the provided job description to extract key skills, experience, and attributes required for the role.\n2. Formulate questions aimed at confirming the candidate's direct experience and work related to these specific criteria from the job description.\n3. Ensure questions are specific to the job description details, yet open enough to evoke detailed responses.\n\n# Input\n\n- {jd}: A structured job description from which key skills, experiences, and qualifications will be identified.\n\n# Output Format\n\nThe output should be a structured list of HR-level interview questions.\n\n- Example Questions:\n  - \"Can you detail your experience working with {{key_skill_1}} as described in the job description?\"\n  - \"The job description mentions experience in {{key_area}}. Can you share a specific project in which you utilized these skills?\"\n  - \"Can you provide examples of how you have demonstrated {{required_attribute}} in previous roles?\"\n\n# Notes\n\n- Tailor questions to verify the candidate's past work and experience directly related to the job description provided.\n- The aim is to assess the candidate's specific capabilities aligning with the job requirements described. (Do not generate more than 3 questions and also make sure that the questions are shorter which will also have shorter answers)"""
    response = client.responses.create(
    model="gpt-4.1-nano",
    input=[
        {
        "role": "system",
        "content": [
            {
            "type": "input_text",
            "text": prompt
            }
        ]
        }
    ],
    text={
        "format": {
        "type": "text"
        }
    },
    reasoning={},
    tools=[],
    temperature=1,
    max_output_tokens=2048,
    top_p=1,
    store=True
    )

    print(response.output[0].content[0].text)
    raw_output = response.output[0].content[0].text
    return raw_output

# get_questions(jd)


provide complete updated code DO NOT CHANGE THE PROMPT

text-embedding-3-small


#!/usr/bin/env python3
"""
Complete usage examples for the Memory Optimized Resume Extractor
"""

import os
from your_resume_extractor_file import MemoryOptimizedResumeExtractor  # Import your class

# =============================================================================
# METHOD 1: Basic Usage with Environment Variables
# =============================================================================

def basic_usage():
    """
    Simplest way to use the extractor - credentials from environment variables
    """
    # Set environment variables first (in terminal or .env file):
    # export OPENAI_API_KEY="sk-your-key-here"
    # export SUPABASE_URL="https://your-project.supabase.co"
    # export SUPABASE_KEY="your-supabase-key"
    
    # Initialize extractor (will auto-load from environment)
    extractor = MemoryOptimizedResumeExtractor(
        max_workers=2,
        batch_size=25,
        max_memory_mb=256
    )
    
    # Your Google Drive spreadsheet link
    drive_url = "https://drive.google.com/file/d/your-spreadsheet-id/view?usp=sharing"
    
    # Process all resumes
    results = []
    for batch_results in extractor.process_drive_spreadsheet_optimized(drive_url):
        results.extend(batch_results)
        print(f"Processed batch of {len(batch_results)} resumes")
    
    print(f"Total processed: {len(results)} resumes")
    return results

# =============================================================================
# METHOD 2: Pass Credentials Explicitly  
# =============================================================================

def explicit_credentials():
    """
    Pass API credentials directly to the constructor
    """
    extractor = MemoryOptimizedResumeExtractor(
        max_workers=2,
        batch_size=25, 
        max_memory_mb=256,
        openai_api_key="sk-your-openai-key-here",
        supabase_url="https://your-project.supabase.co",
        supabase_key="your-supabase-key-here"
    )
    
    drive_url = "https://drive.google.com/file/d/your-spreadsheet-id/view?usp=sharing"
    
    # Process with progress callback and save results
    def my_progress_callback(total_processed, batch_number, stats):
        print(f"Progress: {total_processed} processed, {stats['db_inserted']} saved to DB")
    
    all_results = []
    for batch_results in extractor.process_drive_spreadsheet_optimized(
        drive_url, 
        output_file="results.csv",
        progress_callback=my_progress_callback
    ):
        all_results.extend(batch_results)
    
    return all_results

# =============================================================================
# METHOD 3: Production Usage with Error Handling
# =============================================================================

def production_usage():
    """
    Production-ready usage with comprehensive error handling
    """
    try:
        # Initialize with production settings
        extractor = MemoryOptimizedResumeExtractor(
            max_workers=3,          # Slightly more for production
            batch_size=50,          # Larger batches for efficiency 
            max_memory_mb=512       # More memory if available
        )
        
        # Multiple Google Drive links (if you have multiple spreadsheets)
        drive_urls = [
            "https://drive.google.com/file/d/spreadsheet1-id/view",
            "https://drive.google.com/file/d/spreadsheet2-id/view",
            # Add more URLs as needed
        ]
        
        all_results = []
        
        for i, drive_url in enumerate(drive_urls, 1):
            print(f"\n=== Processing Spreadsheet {i} of {len(drive_urls)} ===")
            
            try:
                output_file = f"results_batch_{i}.csv"
                
                for batch_results in extractor.process_drive_spreadsheet_optimized(
                    drive_url,
                    output_file=output_file
                ):
                    all_results.extend(batch_results)
                    
                    # Log successful batch
                    successful = sum(1 for r in batch_results if r['db_inserted'])
                    print(f"Batch complete: {successful}/{len(batch_results)} saved to database")
                
            except Exception as e:
                print(f"Error processing spreadsheet {i}: {e}")
                continue  # Continue with next spreadsheet
        
        # Final summary
        total_processed = len(all_results)
        successful_db_inserts = sum(1 for r in all_results if r['db_inserted'])
        
        print(f"\n=== FINAL SUMMARY ===")
        print(f"Total Resumes Processed: {total_processed}")
        print(f"Successfully Saved to Database: {successful_db_inserts}")
        print(f"Success Rate: {(successful_db_inserts/total_processed*100):.1f}%")
        
        return all_results
        
    except Exception as e:
        print(f"Critical error in production usage: {e}")
        return []

# =============================================================================
# METHOD 4: Custom Processing with Manual Control
# =============================================================================

def custom_processing():
    """
    Manual control over processing for custom workflows
    """
    extractor = MemoryOptimizedResumeExtractor()
    
    # Read spreadsheet manually first
    drive_url = "https://drive.google.com/file/d/your-id/view"
    
    # Process in custom batches
    for chunk_df in extractor.read_spreadsheet_in_chunks(drive_url, chunk_size=100):
        print(f"Got chunk with {len(chunk_df)} rows")
        
        # Custom filtering (e.g., only process certain candidates)
        filtered_df = chunk_df[chunk_df['status'] == 'active']  # Your custom filter
        
        # Prepare batch data
        batch_data = []
        last_column = filtered_df.columns[-1]  # Resume URL column
        
        for index, row in filtered_df.iterrows():
            batch_data.append({
                'row_number': index,
                'resume_url': row[last_column],
                'original_row': row.to_dict()
            })
        
        # Process the filtered batch
        if batch_data:
            results = extractor.process_batch(batch_data)
            
            # Custom post-processing
            for result in results:
                if result['db_inserted']:
                    print(f"‚úÖ {result['name']} - Resume processed and saved")
                else:
                    print(f"‚ùå {result['name']} - Failed: {result['status']}")

# =============================================================================
# METHOD 5: Simple Script for One-Time Processing
# =============================================================================

def simple_script():
    """
    Simple script for one-time processing of a single spreadsheet
    """
    # Your credentials
    OPENAI_API_KEY = "sk-your-key-here"
    SUPABASE_URL = "https://your-project.supabase.co"
    SUPABASE_KEY = "your-key-here"
    DRIVE_URL = "https://drive.google.com/file/d/your-spreadsheet/view"
    
    # Create extractor
    extractor = MemoryOptimizedResumeExtractor(
        openai_api_key=OPENAI_API_KEY,
        supabase_url=SUPABASE_URL, 
        supabase_key=SUPABASE_KEY
    )
    
    # Process all resumes
    print("Starting resume processing...")
    
    total_count = 0
    success_count = 0
    
    for batch_results in extractor.process_drive_spreadsheet_optimized(DRIVE_URL):
        for result in batch_results:
            total_count += 1
            if result['db_inserted']:
                success_count += 1
                print(f"‚úÖ Processed: {result['name']}")
            else:
                print(f"‚ùå Failed: {result['name']} - {result['status']}")
    
    print(f"\nDone! {success_count}/{total_count} resumes successfully processed")

# =============================================================================
# METHOD 6: Using with Command Line Arguments
# =============================================================================

def command_line_usage():
    """
    Usage with command line arguments
    """
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python script.py <google_drive_url>")
        print("Example: python script.py 'https://drive.google.com/file/d/abc123/view'")
        return
    
    drive_url = sys.argv[1]
    
    extractor = MemoryOptimizedResumeExtractor()
    
    print(f"Processing spreadsheet: {drive_url}")
    
    for batch_results in extractor.process_drive_spreadsheet_optimized(
        drive_url,
        output_file="resume_results.csv"
    ):
        print(f"Processed {len(batch_results)} resumes in this batch")

# =============================================================================
# MAIN EXECUTION
# =============================================================================

if __name__ == "__main__":
    print("Choose a method to run:")
    print("1. Basic usage (environment variables)")
    print("2. Explicit credentials") 
    print("3. Production usage")
    print("4. Custom processing")
    print("5. Simple script")
    print("6. Command line usage")
    
    choice = input("Enter choice (1-6): ")
    
    if choice == "1":
        basic_usage()
    elif choice == "2":
        explicit_credentials()
    elif choice == "3":
        production_usage()
    elif choice == "4":
        custom_processing()
    elif choice == "5":
        simple_script()
    elif choice == "6":
        command_line_usage()
    else:
        print("Invalid choice")

# =============================================================================
# ENVIRONMENT SETUP (.env file example)
# =============================================================================

"""
Create a .env file in your project directory with:

OPENAI_API_KEY=sk-your-openai-key-here
SUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_KEY=your-supabase-anon-key

Then load it with:
from dotenv import load_dotenv
load_dotenv()
"""

# =============================================================================
# INSTALLATION COMMANDS
# =============================================================================

"""
Run these commands to install required packages:

pip install pandas requests PyPDF2 python-docx openpyxl xlrd psutil openai supabase python-dotenv

Or create requirements.txt:
pandas>=1.5.0
requests>=2.28.0
PyPDF2>=3.0.0
python-docx>=0.8.11
openpyxl>=3.0.0
xlrd>=2.0.0
psutil>=5.9.0
openai>=1.0.0
supabase>=1.0.0
python-dotenv>=1.0.0

Then: pip install -r requirements.txt
"""




annotated-types==0.7.0
anyio==4.9.0
blinker==1.9.0
certifi==2025.7.14
charset-normalizer==3.4.2
click==8.2.1
deprecation==2.1.0
distro==1.9.0
Flask==3.1.1
flask-cors==6.0.1
gotrue==2.12.3
h11==0.16.0
h2==4.2.0
hpack==4.1.0
httpcore==1.0.9
httpx==0.28.1
hyperframe==6.1.0
idna==3.10
itsdangerous==2.2.0
Jinja2==3.1.6
jiter==0.10.0
MarkupSafe==3.0.2
openai==1.97.0
packaging==25.0
postgrest==1.1.1
pydantic==2.11.7
pydantic_core==2.33.2
PyJWT==2.10.1
PyMuPDF==1.26.3
python-dateutil==2.9.0.post0
python-dotenv==1.1.1
realtime==2.5.3
requests==2.32.4
six==1.17.0
sniffio==1.3.1
storage3==0.12.0
StrEnum==0.4.15
supabase==2.16.0
supafunc==0.10.1
tqdm==4.67.1
typing-inspection==0.4.1
typing_extensions==4.14.1
urllib3==2.5.0
websockets==15.0.1
Werkzeug==3.1.3
psutil==5.9.8
PyPDF2==3.0.1
python-docx==1.1.2
sentence-transformers==2.2.2
scikit-learn==1.5.2
numpy==1.26.4
scipy==1.14.1
torch==2.3.1
transformers==4.43.3
